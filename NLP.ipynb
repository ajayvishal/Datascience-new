{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283f1832-f4ad-4fd6-9030-c557844b114e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1169be-af63-484b-aa5d-c1bba00c3b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('Eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48daeb63-a09e-4eff-a799-8ac157e66f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shock'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('Shocking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641803d7-105a-4968-9f85-5ef0554cfd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python program'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('Python Programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c17a3395-1ede-4449-8981-d46d8ed96c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lovabl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('Lovable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1657dd4d-b2c5-40b6-bfbe-5db010ae35ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'best', '%%', 'programming', 'language']\n"
     ]
    }
   ],
   "source": [
    "# import WhitespaceTokenizer() method from nltk \n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "     \n",
    "# Create a reference variable for Class WhitespaceTokenizer \n",
    "tk = WhitespaceTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "text= \"Python \\nis\\t best %% programming language\"     \n",
    "# Use tokenize method \n",
    "t = tk.tokenize(text)      \n",
    "print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937bbbd5-43d3-4ac9-ab59-13964ded51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP for', 'Natural..', '.$$&* \\nLanguage', ' Processing']\n"
     ]
    }
   ],
   "source": [
    "# import TabTokenizer() method from nltk \n",
    "from nltk.tokenize import TabTokenizer \n",
    "     \n",
    "# Create a reference variable for Class TabTokenizer \n",
    "tk = TabTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "text = \"NLP for\\tNatural..\\t.$$&* \\nLanguage\\t Processing\"\n",
    "     \n",
    "# Use tokenize method \n",
    "t = tk.tokenize(text) \n",
    "     \n",
    "print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde80971-4c3e-48eb-bd3c-eee20b437cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Py', '*', 'thon', '...$$&*', 'is', 'used', 'for', 'Data', 'Science', 'and', 'ML']\n"
     ]
    }
   ],
   "source": [
    "# import WordPunctTokenizer() method from nltk \n",
    "from nltk.tokenize import WordPunctTokenizer      \n",
    "# Create a reference variable for Class WordPunctTokenizer \n",
    "tk = WordPunctTokenizer()      \n",
    "# Create a string input \n",
    "txt = \"Py*thon...$$&* \\nis\\t used for Data Science and ML\"     \n",
    "# Use tokenize method \n",
    "t= tk.tokenize(txt)      \n",
    "print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb484303-51fe-4a84-a80f-927ca8717913",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (726786916.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    import ConditionalFreqDist() method from nltk\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import ConditionalFreqDist() method from nltk \n",
    "from nltk.probability import ConditionalFreqDist \n",
    "from nltk.tokenize import word_tokenize      \n",
    "# Create a reference variable for Class SExprTokenizer \n",
    "tk = ConditionalFreqDist()      \n",
    "# Create a string input \n",
    "txt = \"Python for Python is Best Language Python is Easy To LEarn\"     \n",
    "for word in word_tokenize(txt):\n",
    "    condition = len(word) \n",
    "    tk[condition][word] += 1     \n",
    "print(tk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13095bef-d5bd-455a-b85a-83e06abd49d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a062b4-6f28-4556-baf9-f940cca51773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ConditionalFreqDist() method from nltk \n",
    "from nltk.probability import ConditionalFreqDist \n",
    "from nltk.tokenize import word_tokenize      \n",
    "# Create a reference variable for Class SExprTokenizer \n",
    "tk = ConditionalFreqDist()      \n",
    "# Create a string input \n",
    "txt = \"Python for Python is best Language\"     \n",
    "for word in word_tokenize(txt):\n",
    "    condition = len(word) \n",
    "    tk[condition][word] += 1     \n",
    "print(tk) \n",
    "tk.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c251d05-0b94-43e2-ad6a-946b2851c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancatserStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "Lanc_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0997fb-0898-40b8-b9df-195fba50d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "# Initialize the Lancaster Stemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "# List of words to be stemmed\n",
    "words = [\"eats\", \"eating\", \"ate\", \"flies\", \"flying\"]\n",
    "\n",
    "# Stem each word and print the results\n",
    "stemmed_words = [Lanc_stemmer.stem(word) for word in words]\n",
    "\n",
    "for original, stemmed in zip(words, stemmed_words):\n",
    "    print(f\"Original: {original}, Stemmed: {stemmed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da2efd-fe6e-46ff-a0e1-1b76384b0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71265399-42c3-4b50-a23e-71831a845eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "help('nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391b11b-2ef3-4428-9ac3-7f14dfe0292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "sent = \"India is a republic nation. We are proud Indians\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bf088-5395-436f-83db-566a17b2b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927e830-f93d-473b-9068-6d850638ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
